# -- (string) Kubernetes config path
# @default -- `"~/.kube/config"`
kubeConfigPath: ""
# -- (string) Kubernetes service host
k8sServiceHost: ""
# -- (string) Kubernetes service port
k8sServicePort: ""


bpf:
  autoMount:
    # -- Enable automatic mount of BPF filesystem
    # When `autoMount` is enabled, the BPF filesystem is mounted at
    # `bpf.root` path on the underlying host and inside the cilium agent pod.
    # If users disable `autoMount`, it's expected that users have mounted
    # bpffs filesystem at the specified `bpf.root` volume, and then the
    # volume will be mounted inside the cilium agent pod at the same path.
    enabled: true
  # -- Configure the mount point for the BPF filesystem
  root: /sys/fs/bpf

  # -- Enables pre-allocation of eBPF map values. This increases
  # memory usage but can reduce latency.
  preallocateMaps: false

  # -- (int) Configure the maximum number of entries in auth map.
  # @default -- `524288`
  authMapMax: ~

  # -- (int) Configure the maximum number of entries in the TCP connection tracking
  # table.
  # @default -- `524288`
  ctTcpMax: ~

  # -- (int) Configure the maximum number of entries for the non-TCP connection
  # tracking table.
  # @default -- `262144`
  ctAnyMax: ~

  # -- Configure the maximum number of service entries in the
  # load balancer maps.
  lbMapMax: 65536

  # -- (int) Configure the maximum number of entries for the NAT table.
  # @default -- `524288`
  natMax: ~

  # -- (int) Configure the maximum number of entries for the neighbor table.
  # @default -- `524288`
  neighMax: ~

  # -- Configure the maximum number of entries in endpoint policy map (per endpoint).
  policyMapMax: 16384

  # -- (float64) Configure auto-sizing for all BPF maps based on available memory.
  # ref: https://docs.cilium.io/en/stable/network/ebpf/maps/
  # @default -- `0.0025`
  mapDynamicSizeRatio: ~

  # -- Configure the level of aggregation for monitor notifications.
  # Valid options are none, low, medium, maximum.
  monitorAggregation: medium

  # -- Configure the typical time between monitor notifications for
  # active connections.
  monitorInterval: "5s"

  # -- Configure which TCP flags trigger notifications when seen for the
  # first time in a connection.
  monitorFlags: "all"

  # -- Allow cluster external access to ClusterIP services.
  lbExternalClusterIP: false

  # -- (bool) Enable native IP masquerade support in eBPF
  # @default -- `false`
  masquerade: ~

  # -- (bool) Configure whether direct routing mode should route traffic via
  # host stack (true) or directly and more efficiently out of BPF (false) if
  # the kernel supports it. The latter has the implication that it will also
  # bypass netfilter in the host namespace.
  # @default -- `false`
  hostLegacyRouting: ~

  # -- (bool) Configure the eBPF-based TPROXY to reduce reliance on iptables rules
  # for implementing Layer 7 policy.
  # @default -- `false`
  tproxy: ~

  # -- (list) Configure explicitly allowed VLAN id's for bpf logic bypass.
  # [0] will allow all VLAN id's without any filtering.
  # @default -- `[]`
  vlanBypass: ~

cni:
  # -- Install the CNI configuration and binary files into the filesystem.
  install: true

  # -- Remove the CNI configuration and binary files on agent shutdown. Enable this
  # if you're removing Cilium from the cluster. Disable this to prevent the CNI
  # configuration file from being removed during agent upgrade, which can cause
  # nodes to go unmanageable.
  uninstall: false

  # -- Configure chaining on top of other CNI plugins. Possible values:
  #  - none
  #  - aws-cni
  #  - flannel
  #  - generic-veth
  #  - portmap
  chainingMode: ~

  # -- A CNI network name in to which the Cilium plugin should be added as a chained plugin.
  # This will cause the agent to watch for a CNI network with this network name. When it is
  # found, this will be used as the basis for Cilium's CNI configuration file. If this is
  # set, it assumes a chaining mode of generic-veth. As a special case, a chaining mode
  # of aws-cni implies a chainingTarget of aws-cni.
  chainingTarget: ~

  # -- Make Cilium take ownership over the `/etc/cni/net.d` directory on the
  # node, renaming all non-Cilium CNI configurations to `*.cilium_bak`.
  # This ensures no Pods can be scheduled using other CNI plugins during Cilium
  # agent downtime.
  exclusive: true

  # -- Configure the log file for CNI logging with retention policy of 7 days.
  # Disable CNI file logging by setting this field to empty explicitly.
  logFile: /var/run/cilium/cilium-cni.log

  # -- Skip writing of the CNI configuration. This can be used if
  # writing of the CNI configuration is performed by external automation.
  customConf: false

  # -- Configure the path to the CNI configuration directory on the host.
  confPath: /etc/cni/net.d

  # -- Configure the path to the CNI binary directory on the host.
  binPath: /opt/cni/bin

  # -- Specify the path to a CNI config to read from on agent start.
  # This can be useful if you want to manage your CNI
  # configuration outside of a Kubernetes environment. This parameter is
  # mutually exclusive with the 'cni.configMap' parameter. The agent will
  # write this to 05-cilium.conflist on startup.
  # readCniConf: /host/etc/cni/net.d/05-sample.conflist.input

  # -- When defined, configMap will mount the provided value as ConfigMap and
  # interpret the cniConf variable as CNI configuration file and write it
  # when the agent starts up
  # configMap: cni-configuration

  # -- Configure the key in the CNI ConfigMap to read the contents of
  # the CNI configuration from.
  configMapKey: cni-config

  # -- Configure the path to where to mount the ConfigMap inside the agent pod.
  confFileMountPath: /tmp/cni-configuration

  # -- Configure the path to where the CNI configuration directory is mounted
  # inside the agent pod.
  hostConfDirMountPath: /host/etc/cni/net.d

  # -- Specifies the resources for the cni initContainer
  resources:
    requests:
      cpu: 100m
      memory: 10Mi

# -- Specify which network interfaces can run the eBPF datapath. This means
# that a packet sent from a pod to a destination outside the cluster will be
# masqueraded (to an output device IPv4 address), if the output device runs the
# program. When not specified, probing will automatically detect devices that have
# a non-local route. This should be used only when autodetection is not suitable.
# devices: ""

# -- Limit iptables-based egress masquerading to interface selector.
# egressMasqueradeInterfaces: ""

# -- Enable setting identity mark for local traffic.
# enableIdentityMark: true

# -- Enable Kubernetes EndpointSlice feature in Cilium if the cluster supports it.
# enableK8sEndpointSlice: true

# -- Enable CiliumEndpointSlice feature.
enableCiliumEndpointSlice: False

eni:
  # -- Enable Elastic Network Interface (ENI) integration.
  enabled: false
  # -- Update ENI Adapter limits from the EC2 API
  updateEC2AdapterLimitViaAPI: true
  # -- Release IPs not used from the ENI
  awsReleaseExcessIPs: false
  # -- Enable ENI prefix delegation
  awsEnablePrefixDelegation: false
  # -- EC2 API endpoint to use
  ec2APIEndpoint: ""
  # -- Tags to apply to the newly created ENIs
  eniTags: {}
  # -- Interval for garbage collection of unattached ENIs. Set to "0s" to disable.
  # @default -- `"5m"`
  gcInterval: ""
  # -- Additional tags attached to ENIs created by Cilium.
  # Dangling ENIs with this tag will be garbage collected
  # @default -- `{"io.cilium/cilium-managed":"true,"io.cilium/cluster-name":"<auto-detected>"}`
  gcTags: {}
  # -- If using IAM role for Service Accounts will not try to
  # inject identity values from cilium-aws kubernetes secret.
  # Adds annotation to service account if managed by Helm.
  # See https://github.com/aws/amazon-eks-pod-identity-webhook
  iamRole: ""
  # -- Filter via subnet IDs which will dictate which subnets are going to be used to create new ENIs
  # Important note: This requires that each instance has an ENI with a matching subnet attached
  # when Cilium is deployed. If you only want to control subnets for ENIs attached by Cilium,
  # use the CNI configuration file settings (cni.customConf) instead.
  subnetIDsFilter: []
  # -- Filter via tags (k=v) which will dictate which subnets are going to be used to create new ENIs
  # Important note: This requires that each instance has an ENI with a matching subnet attached
  # when Cilium is deployed. If you only want to control subnets for ENIs attached by Cilium,
  # use the CNI configuration file settings (cni.customConf) instead.
  subnetTagsFilter: []
  # -- Filter via AWS EC2 Instance tags (k=v) which will dictate which AWS EC2 Instances
  # are going to be used to create new ENIs
  instanceTagsFilter: []

# -- Configure the eBPF-based ip-masq-agent
ipMasqAgent:
  enabled: false
# the config of nonMasqueradeCIDRs
# config:
  # nonMasqueradeCIDRs: []
  # masqLinkLocal: false
  # masqLinkLocalIPv6: false

# -- Enables masquerading of IPv4 traffic leaving the node from endpoints.
enableIPv4Masquerade: true

# -- Enables masquerading of IPv6 traffic leaving the node from endpoints.
enableIPv6Masquerade: true

# -- Enables masquerading to the source of the route for traffic leaving the node from endpoints.
enableMasqueradeRouteSource: false

# -- Enables IPv4 BIG TCP support which increases maximum IPv4 GSO/GRO limits for nodes and pods
enableIPv4BIGTCP: false

# -- Enables IPv6 BIG TCP support which increases maximum IPv6 GSO/GRO limits for nodes and pods
enableIPv6BIGTCP: false


# -- (string) Allows to explicitly specify the IPv4 CIDR for native routing.
# When specified, Cilium assumes networking for this CIDR is preconfigured and
# hands traffic destined for that range to the Linux network stack without
# applying any SNAT.
# Generally speaking, specifying a native routing CIDR implies that Cilium can
# depend on the underlying networking stack to route packets to their
# destination. To offer a concrete example, if Cilium is configured to use
# direct routing and the Kubernetes CIDR is included in the native routing CIDR,
# the user must configure the routes to reach pods, either manually or by
# setting the auto-direct-node-routes flag.
ipv4NativeRoutingCIDR: ""

# -- (string) Allows to explicitly specify the IPv6 CIDR for native routing.
# When specified, Cilium assumes networking for this CIDR is preconfigured and
# hands traffic destined for that range to the Linux network stack without
# applying any SNAT.
# Generally speaking, specifying a native routing CIDR implies that Cilium can
# depend on the underlying networking stack to route packets to their
# destination. To offer a concrete example, if Cilium is configured to use
# direct routing and the Kubernetes CIDR is included in the native routing CIDR,
# the user must configure the routes to reach pods, either manually or by
# setting the auto-direct-node-routes flag.
ipv6NativeRoutingCIDR: ""

# -- Tunneling protocol to use in tunneling mode and for ad-hoc tunnels.
# Possible values:
#   - ""
#   - vxlan
#   - geneve
# @default -- `"vxlan"`
tunnelProtocol: ""

# -- Enable native-routing mode or tunneling mode.
# Possible values:
#   - ""
#   - native
#   - tunnel
# @default -- `"tunnel"`
routingMode: ""

# -- Configure VXLAN and Geneve tunnel port.
# @default -- Port 8472 for VXLAN, Port 6081 for Geneve
tunnelPort: 0



# -- Configure the underlying network MTU to overwrite auto-detected MTU.
MTU: 0

# -- Disable the usage of CiliumEndpoint CRD.
disableEndpointCRD: false

wellKnownIdentities:
  # -- Enable the use of well-known identities.
  enabled: false

etcd:
  # -- Enable etcd mode for the agent.
  enabled: false

  # -- cilium-etcd-operator image.
  image:
    override: ~
    repository: "quay.io/cilium/cilium-etcd-operator"
    tag: "v2.0.7"
    digest: "sha256:04b8327f7f992693c2cb483b999041ed8f92efc8e14f2a5f3ab95574a65ea2dc"
    useDigest: true
    pullPolicy: "IfNotPresent"

  # -- The priority class to use for cilium-etcd-operator
  priorityClassName: ""

  # -- Additional cilium-etcd-operator container arguments.
  extraArgs: []

  # -- Additional cilium-etcd-operator volumes.
  extraVolumes: []

  # -- Additional cilium-etcd-operator volumeMounts.
  extraVolumeMounts: []

  # -- Node tolerations for cilium-etcd-operator scheduling to nodes with taints
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations:
  - operator: Exists
    # - key: "key"
    #   operator: "Equal|Exists"
    #   value: "value"
    #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  # -- Pod topology spread constraints for cilium-etcd-operator
  topologySpreadConstraints: []
    # - maxSkew: 1
    #   topologyKey: topology.kubernetes.io/zone
    #   whenUnsatisfiable: DoNotSchedule

  # -- Node labels for cilium-etcd-operator pod assignment
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
  nodeSelector:
    kubernetes.io/os: linux

  # -- Annotations to be added to all top-level etcd-operator objects (resources under templates/etcd-operator)
  annotations: {}

  # -- Security context to be added to cilium-etcd-operator pods
  podSecurityContext: {}

  # -- Annotations to be added to cilium-etcd-operator pods
  podAnnotations: {}

  # -- Labels to be added to cilium-etcd-operator pods
  podLabels: {}

  # PodDisruptionBudget settings
  podDisruptionBudget:
    # -- enable PodDisruptionBudget
    # ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
    enabled: false
    # -- Minimum number/percentage of pods that should remain scheduled.
    # When it's set, maxUnavailable must be disabled by `maxUnavailable: null`
    minAvailable: null
    # -- Maximum number/percentage of pods that may be made unavailable
    maxUnavailable: 1

  # -- cilium-etcd-operator resource limits & requests
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources: {}
    # limits:
    #   cpu: 4000m
    #   memory: 4Gi
    # requests:
    #   cpu: 100m
    #   memory: 512Mi

  # -- Security context to be added to cilium-etcd-operator pods
  securityContext: {}
    # runAsUser: 0

  # -- cilium-etcd-operator update strategy
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1

  # -- If etcd is behind a k8s service set this option to true so that Cilium
  # does the service translation automatically without requiring a DNS to be
  # running.
  k8sService: false

  # -- Cluster domain for cilium-etcd-operator.
  clusterDomain: cluster.local

  # -- List of etcd endpoints (not needed when using managed=true).
  endpoints:
    - https://CHANGE-ME:2379

  # -- Enable use of TLS/SSL for connectivity to etcd. (auto-enabled if
  # managed=true)
  ssl: false

operator:
  # -- Enable the cilium-operator component (required).
  enabled: true

  # -- Roll out cilium-operator pods automatically when configmap is updated.
  rollOutPods: false

  # -- cilium-operator image.
  image:
    override: ~
    repository: "quay.io/cilium/operator"
    tag: "v1.15.0"
    # operator-generic-digest
    genericDigest: "sha256:e26ecd316e742e4c8aa1e302ba8b577c2d37d114583d6c4cdd2b638493546a79"
    # operator-azure-digest
    azureDigest: "sha256:498a9e940cddd4e58d401a13005b0784ed9597bfe1e5cf2f52b6ba9ccceee768"
    # operator-aws-digest
    awsDigest: "sha256:cf45167a8bb336c763046553c6a97c0d7f12f7e2a498dfb2340fa27832a81b3a"
    # operator-alibabacloud-digest
    alibabacloudDigest: "sha256:ee03349caef5519f8e9123132cf17c85b771f8fff095c57f00a2af8bb3224b79"
    useDigest: true
    pullPolicy: "IfNotPresent"
    suffix: ""

  # -- Number of replicas to run for the cilium-operator deployment
  replicas: 2

  # -- The priority class to use for cilium-operator
  priorityClassName: ""

  # -- DNS policy for Cilium operator pods.
  # Ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy
  dnsPolicy: ""

  # -- cilium-operator update strategy
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 50%

  # -- Affinity for cilium-operator
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - topologyKey: kubernetes.io/hostname
        labelSelector:
          matchLabels:
            io.cilium/app: operator

  # -- Pod topology spread constraints for cilium-operator
  topologySpreadConstraints: []
    # - maxSkew: 1
    #   topologyKey: topology.kubernetes.io/zone
    #   whenUnsatisfiable: DoNotSchedule

  # -- Node labels for cilium-operator pod assignment
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
  nodeSelector:
    kubernetes.io/os: linux

  # -- Node tolerations for cilium-operator scheduling to nodes with taints
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations:
  - operator: Exists
    # - key: "key"
    #   operator: "Equal|Exists"
    #   value: "value"
    #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  # -- Additional cilium-operator container arguments.
  extraArgs: []

  # -- Additional cilium-operator environment variables.
  extraEnv: []

  # -- Additional cilium-operator hostPath mounts.
  extraHostPathMounts: []
    # - name: host-mnt-data
    #   mountPath: /host/mnt/data
    #   hostPath: /mnt/data
    #   hostPathType: Directory
    #   readOnly: true
    #   mountPropagation: HostToContainer

  # -- Additional cilium-operator volumes.
  extraVolumes: []

  # -- Additional cilium-operator volumeMounts.
  extraVolumeMounts: []

  # -- Annotations to be added to all top-level cilium-operator objects (resources under templates/cilium-operator)
  annotations: {}

  # -- Security context to be added to cilium-operator pods
  podSecurityContext: {}

  # -- Annotations to be added to cilium-operator pods
  podAnnotations: {}

  # -- Labels to be added to cilium-operator pods
  podLabels: {}

  # PodDisruptionBudget settings
  podDisruptionBudget:
    # -- enable PodDisruptionBudget
    # ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
    enabled: false
    # -- Minimum number/percentage of pods that should remain scheduled.
    # When it's set, maxUnavailable must be disabled by `maxUnavailable: null`
    minAvailable: null
    # -- Maximum number/percentage of pods that may be made unavailable
    maxUnavailable: 1

  # -- cilium-operator resource limits & requests
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources: {}
    # limits:
    #   cpu: 1000m
    #   memory: 1Gi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  # -- Security context to be added to cilium-operator pods
  securityContext: {}
    # runAsUser: 0

  # -- Interval for endpoint garbage collection.
  endpointGCInterval: "5m0s"

  # -- Interval for cilium node garbage collection.
  nodeGCInterval: "5m0s"

  # -- Skip CNP node status clean up at operator startup.
  skipCNPStatusStartupClean: false

  # -- Interval for identity garbage collection.
  identityGCInterval: "15m0s"

  # -- Timeout for identity heartbeats.
  identityHeartbeatTimeout: "30m0s"

  pprof:
    # -- Enable pprof for cilium-operator
    enabled: false
    # -- Configure pprof listen address for cilium-operator
    address: localhost
    # -- Configure pprof listen port for cilium-operator
    port: 6061

  # -- Enable prometheus metrics for cilium-operator on the configured port at
  # /metrics
  prometheus:
    enabled: true
    port: 9963
    serviceMonitor:
      # -- Enable service monitors.
      # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
      enabled: false
      # -- Labels to add to ServiceMonitor cilium-operator
      labels: {}
      # -- Annotations to add to ServiceMonitor cilium-operator
      annotations: {}
      # -- jobLabel to add for ServiceMonitor cilium-operator
      jobLabel: ""
      # -- Interval for scrape metrics.
      interval: "10s"
      # -- Relabeling configs for the ServiceMonitor cilium-operator
      relabelings: ~
      # -- Metrics relabeling configs for the ServiceMonitor cilium-operator
      metricRelabelings: ~

  # -- Grafana dashboards for cilium-operator
  # grafana can import dashboards based on the label and value
  # ref: https://github.com/grafana/helm-charts/tree/main/charts/grafana#sidecar-for-dashboards
  dashboards:
    enabled: false
    label: grafana_dashboard
    namespace: ~
    labelValue: "1"
    annotations: {}

  # -- Skip CRDs creation for cilium-operator
  skipCRDCreation: false

  # -- Remove Cilium node taint from Kubernetes nodes that have a healthy Cilium
  # pod running.
  removeNodeTaints: true

  # -- Taint nodes where Cilium is scheduled but not running. This prevents pods
  # from being scheduled to nodes where Cilium is not the default CNI provider.
  # @default -- same as removeNodeTaints
  setNodeTaints: ~

  # -- Set Node condition NetworkUnavailable to 'false' with the reason
  # 'CiliumIsUp' for nodes that have a healthy Cilium pod.
  setNodeNetworkStatus: true

  unmanagedPodWatcher:
    # -- Restart any pod that are not managed by Cilium.
    restart: true
    # -- Interval, in seconds, to check if there are any pods that are not
    # managed by Cilium.
    intervalSeconds: 15

nodeinit:
  # -- Enable the node initialization DaemonSet
  enabled: false

  # -- node-init image.
  image:
    override: ~
    repository: "quay.io/cilium/startup-script"
    tag: "62093c5c233ea914bfa26a10ba41f8780d9b737f"
    pullPolicy: "IfNotPresent"

  # -- The priority class to use for the nodeinit pod.
  priorityClassName: ""

  # -- node-init update strategy
  updateStrategy:
    type: RollingUpdate

  # -- Additional nodeinit environment variables.
  extraEnv: []

  # -- Additional nodeinit volumes.
  extraVolumes: []

  # -- Additional nodeinit volumeMounts.
  extraVolumeMounts: []

  # -- Affinity for cilium-nodeinit
  affinity: {}

  # -- Node labels for nodeinit pod assignment
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
  nodeSelector:
    kubernetes.io/os: linux

  # -- Node tolerations for nodeinit scheduling to nodes with taints
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations:
  - operator: Exists
    # - key: "key"
    #   operator: "Equal|Exists"
    #   value: "value"
    #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  # -- Annotations to be added to all top-level nodeinit objects (resources under templates/cilium-nodeinit)
  annotations: {}

  # -- Annotations to be added to node-init pods.
  podAnnotations: {}

  # -- Labels to be added to node-init pods.
  podLabels: {}

  # -- nodeinit resource limits & requests
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources:
    requests:
      cpu: 100m
      memory: 100Mi

  # -- Security context to be added to nodeinit pods.
  securityContext:
    privileged: false
    seLinuxOptions:
      level: 's0'
      # Running with spc_t since we have removed the privileged mode.
      # Users can change it to a different type as long as they have the
      # type available on the system.
      type: 'spc_t'
    capabilities:
      add:
        # Used in iptables. Consider removing once we are iptables-free
        - SYS_MODULE
        # Used for nsenter
        - NET_ADMIN
        - SYS_ADMIN
        - SYS_CHROOT
        - SYS_PTRACE

  # -- bootstrapFile is the location of the file where the bootstrap timestamp is
  # written by the node-init DaemonSet
  bootstrapFile: "/tmp/cilium-bootstrap.d/cilium-bootstrap-time"

  # -- startup offers way to customize startup nodeinit script (pre and post position)
  startup:
   preScript: ""
   postScript: ""
  # -- prestop offers way to customize prestop nodeinit script (pre and post position)
  prestop:
   preScript: ""
   postScript: ""

preflight:
  # -- Enable Cilium pre-flight resources (required for upgrade)
  enabled: false

  # -- Cilium pre-flight image.
  image:
    override: ~
    repository: "quay.io/cilium/cilium"
    tag: "v1.15.0"
    # cilium-digest
    digest: "sha256:9cfd6a0a3a964780e73a11159f93cc363e616f7d9783608f62af6cfdf3759619"
    useDigest: true
    pullPolicy: "IfNotPresent"

  # -- The priority class to use for the preflight pod.
  priorityClassName: ""

  # -- preflight update strategy
  updateStrategy:
    type: RollingUpdate

  # -- Additional preflight environment variables.
  extraEnv: []

  # -- Additional preflight volumes.
  extraVolumes: []

  # -- Additional preflight volumeMounts.
  extraVolumeMounts: []

  # -- Affinity for cilium-preflight
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - topologyKey: kubernetes.io/hostname
        labelSelector:
          matchLabels:
            k8s-app: cilium

  # -- Node labels for preflight pod assignment
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
  nodeSelector:
    kubernetes.io/os: linux

  # -- Node tolerations for preflight scheduling to nodes with taints
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations:
  - key: node.kubernetes.io/not-ready
    effect: NoSchedule
  - key: node-role.kubernetes.io/master
    effect: NoSchedule
  - key: node-role.kubernetes.io/control-plane
    effect: NoSchedule
  - key: node.cloudprovider.kubernetes.io/uninitialized
    effect: NoSchedule
    value: "true"
  - key: CriticalAddonsOnly
    operator: "Exists"
    # - key: "key"
    #   operator: "Equal|Exists"
    #   value: "value"
    #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  # -- Annotations to be added to all top-level preflight objects (resources under templates/cilium-preflight)
  annotations: {}

  # -- Security context to be added to preflight pods.
  podSecurityContext: {}

  # -- Annotations to be added to preflight pods
  podAnnotations: {}

  # -- Labels to be added to the preflight pod.
  podLabels: {}

  # PodDisruptionBudget settings
  podDisruptionBudget:
    # -- enable PodDisruptionBudget
    # ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
    enabled: false
    # -- Minimum number/percentage of pods that should remain scheduled.
    # When it's set, maxUnavailable must be disabled by `maxUnavailable: null`
    minAvailable: null
    # -- Maximum number/percentage of pods that may be made unavailable
    maxUnavailable: 1

  # -- preflight resource limits & requests
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources: {}
    # limits:
    #   cpu: 4000m
    #   memory: 4Gi
    # requests:
    #   cpu: 100m
    #   memory: 512Mi

  # -- Security context to be added to preflight pods
  securityContext: {}
    # runAsUser: 0

  # -- Path to write the `--tofqdns-pre-cache` file to.
  tofqdnsPreCache: ""

  # -- Configure termination grace period for preflight Deployment and DaemonSet.
  terminationGracePeriodSeconds: 1

  # -- By default we should always validate the installed CNPs before upgrading
  # Cilium. This will make sure the user will have the policies deployed in the
  # cluster with the right schema.
  validateCNPs: true

# -- Explicitly enable or disable priority class.
# .Capabilities.KubeVersion is unsettable in `helm template` calls,
# it depends on k8s libraries version that Helm was compiled against.
# This option allows to explicitly disable setting the priority class, which
# is useful for rendering charts for gke clusters in advance.
enableCriticalPriorityClass: true

# disableEnvoyVersionCheck removes the check for Envoy, which can be useful
# on AArch64 as the images do not currently ship a version of Envoy.
#disableEnvoyVersionCheck: false

clustermesh:
  # -- Deploy clustermesh-apiserver for clustermesh
  useAPIServer: false
  # -- The maximum number of clusters to support in a ClusterMesh. This value
  # cannot be changed on running clusters, and all clusters in a ClusterMesh
  # must be configured with the same value. Values > 255 will decrease the
  # maximum allocatable cluster-local identities.
  # Supported values are 255 and 511.
  maxConnectedClusters: 255

  # -- Annotations to be added to all top-level clustermesh objects (resources under templates/clustermesh-apiserver and templates/clustermesh-config)
  annotations: {}

  # -- Clustermesh explicit configuration.
  config:
    # -- Enable the Clustermesh explicit configuration.
    enabled: false
    # -- Default dns domain for the Clustermesh API servers
    # This is used in the case cluster addresses are not provided
    # and IPs are used.
    domain: mesh.cilium.io
    # -- List of clusters to be peered in the mesh.
    clusters: []
    # clusters:
    # # -- Name of the cluster
    # - name: cluster1
    # # -- Address of the cluster, use this if you created DNS records for
    # # the cluster Clustermesh API server.
    #   address: cluster1.mesh.cilium.io
    # # -- Port of the cluster Clustermesh API server.
    #   port: 2379
    # # -- IPs of the cluster Clustermesh API server, use multiple ones when
    # # you have multiple IPs to access the Clustermesh API server.
    #   ips:
    #   - 172.18.255.201
    # # -- base64 encoded PEM values for the cluster client certificate, private key and certificate authority.
    # # These fields can (and should) be omitted in case the CA is shared across clusters. In that case, the
    # # "remote" private key and certificate available in the local cluster are automatically used instead.
    #   tls:
    #     cert: ""
    #     key: ""
    #     caCert: ""

  apiserver:
    # -- Clustermesh API server image.
    image:
      override: ~
      repository: "quay.io/cilium/clustermesh-apiserver"
      tag: "v1.15.0"
      # clustermesh-apiserver-digest
      digest: "sha256:43feb49dfbaa82388dc653ce12c7626ce40ae375e9853d71b9f5cff0ce61d54a"
      useDigest: true
      pullPolicy: "IfNotPresent"

    etcd:
      # The etcd binary is included in the clustermesh API server image, so the same image from above is reused.
      # Independent override isn't supported, because clustermesh-apiserver is tested against the etcd version it is
      # built with.

      # -- Specifies the resources for etcd container in the apiserver
      resources: {}
      #   requests:
      #     cpu: 200m
      #     memory: 256Mi
      #   limits:
      #     cpu: 1000m
      #     memory: 256Mi

      # -- Security context to be added to clustermesh-apiserver etcd containers
      securityContext: {}

      # -- lifecycle setting for the etcd container
      lifecycle: {}

      init:
        # -- Specifies the resources for etcd init container in the apiserver
        resources: {}
        #   requests:
        #     cpu: 100m
        #     memory: 100Mi
        #   limits:
        #     cpu: 100m
        #     memory: 100Mi

        # -- Additional arguments to `clustermesh-apiserver etcdinit`.
        extraArgs: []

        # -- Additional environment variables to `clustermesh-apiserver etcdinit`.
        extraEnv: []

    kvstoremesh:
      # -- Enable KVStoreMesh. KVStoreMesh caches the information retrieved
      # from the remote clusters in the local etcd instance.
      enabled: false

      # -- Additional KVStoreMesh arguments.
      extraArgs: []

      # -- Additional KVStoreMesh environment variables.
      extraEnv: []

      # -- Resource requests and limits for the KVStoreMesh container
      resources: {}
        # requests:
        #   cpu: 100m
        #   memory: 64Mi
        # limits:
        #   cpu: 1000m
        #   memory: 1024M

      # -- Additional KVStoreMesh volumeMounts.
      extraVolumeMounts: []

      # -- KVStoreMesh Security context
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
            - ALL

      # -- lifecycle setting for the KVStoreMesh container
      lifecycle: {}

    service:
      # -- The type of service used for apiserver access.
      type: NodePort
      # -- Optional port to use as the node port for apiserver access.
      #
      # WARNING: make sure to configure a different NodePort in each cluster if
      # kube-proxy replacement is enabled, as Cilium is currently affected by a known
      # bug (#24692) when NodePorts are handled by the KPR implementation. If a service
      # with the same NodePort exists both in the local and the remote cluster, all
      # traffic originating from inside the cluster and targeting the corresponding
      # NodePort will be redirected to a local backend, regardless of whether the
      # destination node belongs to the local or the remote cluster.
      nodePort: 32379
      # -- Optional loadBalancer IP address to use with type LoadBalancer.
      # loadBalancerIP:

      # -- Annotations for the clustermesh-apiserver
      # For GKE LoadBalancer, use annotation cloud.google.com/load-balancer-type: "Internal"
      # For EKS LoadBalancer, use annotation service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0
      annotations: {}

      # -- The externalTrafficPolicy of service used for apiserver access.
      externalTrafficPolicy:

      # -- The internalTrafficPolicy of service used for apiserver access.
      internalTrafficPolicy:

    # -- Number of replicas run for the clustermesh-apiserver deployment.
    replicas: 1

    # -- lifecycle setting for the apiserver container
    lifecycle: {}

    # -- terminationGracePeriodSeconds for the clustermesh-apiserver deployment
    terminationGracePeriodSeconds: 30

    # -- Additional clustermesh-apiserver arguments.
    extraArgs: []

    # -- Additional clustermesh-apiserver environment variables.
    extraEnv: []

    # -- Additional clustermesh-apiserver volumes.
    extraVolumes: []

    # -- Additional clustermesh-apiserver volumeMounts.
    extraVolumeMounts: []

    # -- Security context to be added to clustermesh-apiserver containers
    securityContext: {}

    # -- Security context to be added to clustermesh-apiserver pods
    podSecurityContext: {}

    # -- Annotations to be added to clustermesh-apiserver pods
    podAnnotations: {}

    # -- Labels to be added to clustermesh-apiserver pods
    podLabels: {}

    # PodDisruptionBudget settings
    podDisruptionBudget:
      # -- enable PodDisruptionBudget
      # ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
      enabled: false
      # -- Minimum number/percentage of pods that should remain scheduled.
      # When it's set, maxUnavailable must be disabled by `maxUnavailable: null`
      minAvailable: null
      # -- Maximum number/percentage of pods that may be made unavailable
      maxUnavailable: 1

    # -- Resource requests and limits for the clustermesh-apiserver container of the clustermesh-apiserver deployment, such as
    #     resources:
    #       limits:
    #         cpu: 1000m
    #         memory: 1024M
    #       requests:
    #         cpu: 100m
    #         memory: 64Mi
    # -- Resource requests and limits for the clustermesh-apiserver
    resources: {}
      # requests:
      #   cpu: 100m
      #   memory: 64Mi
      # limits:
      #   cpu: 1000m
      #   memory: 1024M

    # -- Affinity for clustermesh.apiserver
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - topologyKey: kubernetes.io/hostname
          labelSelector:
            matchLabels:
              k8s-app: clustermesh-apiserver

    # -- Pod topology spread constraints for clustermesh-apiserver
    topologySpreadConstraints: []
      # - maxSkew: 1
      #   topologyKey: topology.kubernetes.io/zone
      #   whenUnsatisfiable: DoNotSchedule

    # -- Node labels for pod assignment
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
    nodeSelector:
      kubernetes.io/os: linux

    # -- Node tolerations for pod assignment on nodes with taints
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
    tolerations: []

    # -- clustermesh-apiserver update strategy
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        maxUnavailable: 1

    # -- The priority class to use for clustermesh-apiserver
    priorityClassName: ""

    tls:
      # -- Configure the clustermesh authentication mode.
      # Supported values:
      # - legacy:     All clusters access remote clustermesh instances with the same
      #               username (i.e., remote). The "remote" certificate must be
      #               generated with CN=remote if provided manually.
      # - migration:  Intermediate mode required to upgrade from legacy to cluster
      #               (and vice versa) with no disruption. Specifically, it enables
      #               the creation of the per-cluster usernames, while still using
      #               the common one for authentication. The "remote" certificate must
      #               be generated with CN=remote if provided manually (same as legacy).
      # - cluster:    Each cluster accesses remote etcd instances with a username
      #               depending on the local cluster name (i.e., remote-<cluster-name>).
      #               The "remote" certificate must be generated with CN=remote-<cluster-name>
      #               if provided manually. Cluster mode is meaningful only when the same
      #               CA is shared across all clusters part of the mesh.
      authMode: legacy

      # -- Configure automatic TLS certificates generation.
      # A Kubernetes CronJob is used the generate any
      # certificates not provided by the user at installation
      # time.
      auto:
        # -- When set to true, automatically generate a CA and certificates to
        # enable mTLS between clustermesh-apiserver and external workload instances.
        # If set to false, the certs to be provided by setting appropriate values below.
        enabled: true
        # Sets the method to auto-generate certificates. Supported values:
        # - helm:         This method uses Helm to generate all certificates.
        # - cronJob:      This method uses a Kubernetes CronJob the generate any
        #                 certificates not provided by the user at installation
        #                 time.
        # - certmanager:  This method use cert-manager to generate & rotate certificates.
        method: helm
        # -- Generated certificates validity duration in days.
        certValidityDuration: 1095
        # -- Schedule for certificates regeneration (regardless of their expiration date).
        # Only used if method is "cronJob". If nil, then no recurring job will be created.
        # Instead, only the one-shot job is deployed to generate the certificates at
        # installation time.
        #
        # Due to the out-of-band distribution of client certs to external workloads the
        # CA is (re)regenerated only if it is not provided as a helm value and the k8s
        # secret is manually deleted.
        #
        # Defaults to none. Commented syntax gives midnight of the first day of every
        # fourth month. For syntax, see
        # https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#schedule-syntax
        # schedule: "0 0 1 */4 *"

        # [Example]
        # certManagerIssuerRef:
        #   group: cert-manager.io
        #   kind: ClusterIssuer
        #   name: ca-issuer
        # -- certmanager issuer used when clustermesh.apiserver.tls.auto.method=certmanager.
        certManagerIssuerRef: {}
      # -- base64 encoded PEM values for the clustermesh-apiserver server certificate and private key.
      # Used if 'auto' is not enabled.
      server:
        cert: ""
        key: ""
        # -- Extra DNS names added to certificate when it's auto generated
        extraDnsNames: []
        # -- Extra IP addresses added to certificate when it's auto generated
        extraIpAddresses: []
      # -- base64 encoded PEM values for the clustermesh-apiserver admin certificate and private key.
      # Used if 'auto' is not enabled.
      admin:
        cert: ""
        key: ""
      # -- base64 encoded PEM values for the clustermesh-apiserver client certificate and private key.
      # Used if 'auto' is not enabled.
      client:
        cert: ""
        key: ""
      # -- base64 encoded PEM values for the clustermesh-apiserver remote cluster certificate and private key.
      # Used if 'auto' is not enabled.
      remote:
        cert: ""
        key: ""

    # clustermesh-apiserver Prometheus metrics configuration
    metrics:
      # -- Enables exporting apiserver metrics in OpenMetrics format.
      enabled: true
      # -- Configure the port the apiserver metric server listens on.
      port: 9962

      kvstoremesh:
        # -- Enables exporting KVStoreMesh metrics in OpenMetrics format.
        enabled: true
         # -- Configure the port the KVStoreMesh metric server listens on.
        port: 9964

      etcd:
        # -- Enables exporting etcd metrics in OpenMetrics format.
        enabled: true
        # -- Set level of detail for etcd metrics; specify 'extensive' to include server side gRPC histogram metrics.
        mode: basic
        # -- Configure the port the etcd metric server listens on.
        port: 9963

      serviceMonitor:
        # -- Enable service monitor.
        # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
        enabled: false
        # -- Labels to add to ServiceMonitor clustermesh-apiserver
        labels: {}
        # -- Annotations to add to ServiceMonitor clustermesh-apiserver
        annotations: {}
        # -- Specify the Kubernetes namespace where Prometheus expects to find
        # service monitors configured.
        # namespace: ""

        # -- Interval for scrape metrics (apiserver metrics)
        interval: "10s"
        # -- Relabeling configs for the ServiceMonitor clustermesh-apiserver (apiserver metrics)
        relabelings: ~
        # -- Metrics relabeling configs for the ServiceMonitor clustermesh-apiserver (apiserver metrics)
        metricRelabelings: ~

        kvstoremesh:
          # -- Interval for scrape metrics (KVStoreMesh metrics)
          interval: "10s"
          # -- Relabeling configs for the ServiceMonitor clustermesh-apiserver (KVStoreMesh metrics)
          relabelings: ~
          # -- Metrics relabeling configs for the ServiceMonitor clustermesh-apiserver (KVStoreMesh metrics)
          metricRelabelings: ~

        etcd:
          # -- Interval for scrape metrics (etcd metrics)
          interval: "10s"
          # -- Relabeling configs for the ServiceMonitor clustermesh-apiserver (etcd metrics)
          relabelings: ~
          # -- Metrics relabeling configs for the ServiceMonitor clustermesh-apiserver (etcd metrics)
          metricRelabelings: ~

# -- Configure external workloads support
externalWorkloads:
  # -- Enable support for external workloads, such as VMs (false by default).
  enabled: false

# -- Configure cgroup related configuration
cgroup:
  autoMount:
    # -- Enable auto mount of cgroup2 filesystem.
    # When `autoMount` is enabled, cgroup2 filesystem is mounted at
    # `cgroup.hostRoot` path on the underlying host and inside the cilium agent pod.
    # If users disable `autoMount`, it's expected that users have mounted
    # cgroup2 filesystem at the specified `cgroup.hostRoot` volume, and then the
    # volume will be mounted inside the cilium agent pod at the same path.
    enabled: true
    # -- Init Container Cgroup Automount resource limits & requests
    resources: {}
      # limits:
      #   cpu: 100m
      #   memory: 128Mi
      # requests:
      #   cpu: 100m
      #   memory: 128Mi
  # -- Configure cgroup root where cgroup2 filesystem is mounted on the host (see also: `cgroup.autoMount`)
  hostRoot: /run/cilium/cgroupv2

# -- Configure whether to enable auto detect of terminating state for endpoints
# in order to support graceful termination.
enableK8sTerminatingEndpoint: true

# -- Configure whether to unload DNS policy rules on graceful shutdown
# dnsPolicyUnloadOnShutdown: false

# -- Configure the key of the taint indicating that Cilium is not ready on the node.
# When set to a value starting with `ignore-taint.cluster-autoscaler.kubernetes.io/`, the Cluster Autoscaler will ignore the taint on its decisions, allowing the cluster to scale up.
agentNotReadyTaintKey: "node.cilium.io/agent-not-ready"

dnsProxy:
  # -- DNS response code for rejecting DNS requests, available options are '[nameError refused]'.
  dnsRejectResponseCode: refused
  # -- Allow the DNS proxy to compress responses to endpoints that are larger than 512 Bytes or the EDNS0 option, if present.
  enableDnsCompression: true
  # -- Maximum number of IPs to maintain per FQDN name for each endpoint.
  endpointMaxIpPerHostname: 50
  # -- Time during which idle but previously active connections with expired DNS lookups are still considered alive.
  idleConnectionGracePeriod: 0s
  # -- Maximum number of IPs to retain for expired DNS lookups with still-active connections.
  maxDeferredConnectionDeletes: 10000
  # -- The minimum time, in seconds, to use DNS data for toFQDNs policies. If
  # the upstream DNS server returns a DNS record with a shorter TTL, Cilium
  # overwrites the TTL with this value. Setting this value to zero means that
  # Cilium will honor the TTLs returned by the upstream DNS server.
  minTtl: 0
  # -- DNS cache data at this path is preloaded on agent startup.
  preCache: ""
  # -- Global port on which the in-agent DNS proxy should listen. Default 0 is a OS-assigned port.
  proxyPort: 0
  # -- The maximum time the DNS proxy holds an allowed DNS response before sending it along. Responses are sent as soon as the datapath is updated with the new IP information.
  proxyResponseMaxDelay: 100ms
  # -- DNS proxy operation mode (true/false, or unset to use version dependent defaults)
  # enableTransparentMode: true

# -- SCTP Configuration Values
sctp:
  # -- Enable SCTP support. NOTE: Currently, SCTP support does not support rewriting ports or multihoming.
  enabled: false

# Configuration for types of authentication for Cilium (beta)
authentication:
  # -- Enable authentication processing and garbage collection.
  # Note that if disabled, policy enforcement will still block requests that require authentication.
  # But the resulting authentication requests for these requests will not be processed, therefore the requests not be allowed.
  enabled: true
  # -- Buffer size of the channel Cilium uses to receive authentication events from the signal map.
  queueSize: 1024
  # -- Buffer size of the channel Cilium uses to receive certificate expiration events from auth handlers.
  rotatedIdentitiesQueueSize: 1024
    # -- Interval for garbage collection of auth map entries.
  gcInterval: "5m0s"
  # Configuration for Cilium's service-to-service mutual authentication using TLS handshakes.
  # Note that this is not full mTLS support without also enabling encryption of some form.
  # Current encryption options are Wireguard or IPSec, configured in encryption block above.
  mutual:
    # -- Port on the agent where mutual authentication handshakes between agents will be performed
    port: 4250
    # -- Timeout for connecting to the remote node TCP socket
    connectTimeout: 5s
    # Settings for SPIRE
    spire:
      # -- Enable SPIRE integration (beta)
      enabled: false
      # -- Annotations to be added to all top-level spire objects (resources under templates/spire)
      annotations: {}
      # Settings to control the SPIRE installation and configuration
      install:
        # -- Enable SPIRE installation.
        # This will only take effect only if authentication.mutual.spire.enabled is true
        enabled: true
        # -- SPIRE namespace to install into
        namespace: cilium-spire
        # -- SPIRE namespace already exists. Set to true if Helm should not create, manage, and import the SPIRE namespace.
        existingNamespace: false
        # -- init container image of SPIRE agent and server
        initImage:
          override: ~
          repository: "docker.io/library/busybox"
          tag: "1.36.1"
          digest: "sha256:223ae047b1065bd069aac01ae3ac8088b3ca4a527827e283b85112f29385fb1b"
          useDigest: true
          pullPolicy: "IfNotPresent"
        # SPIRE agent configuration
        agent:
          # -- SPIRE agent image
          image:
            override: ~
            repository: "ghcr.io/spiffe/spire-agent"
            tag: "1.8.5"
            digest: "sha256:99405637647968245ff9fe215f8bd2bd0ea9807be9725f8bf19fe1b21471e52b"
            useDigest: true
            pullPolicy: "IfNotPresent"
          # -- SPIRE agent service account
          serviceAccount:
            create: true
            name: spire-agent
          # -- SPIRE agent annotations
          annotations: {}
          # -- SPIRE agent labels
          labels: {}
          # -- SPIRE Workload Attestor kubelet verification.
          skipKubeletVerification: true
          # -- SPIRE agent tolerations configuration
          # By default it follows the same tolerations as the agent itself
          # to allow the Cilium agent on this node to connect to SPIRE.
          # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
          tolerations:
          - key: node.kubernetes.io/not-ready
            effect: NoSchedule
          - key: node-role.kubernetes.io/master
            effect: NoSchedule
          - key: node-role.kubernetes.io/control-plane
            effect: NoSchedule
          - key: node.cloudprovider.kubernetes.io/uninitialized
            effect: NoSchedule
            value: "true"
          - key: CriticalAddonsOnly
            operator: "Exists"
          # -- SPIRE agent affinity configuration
          affinity: {}
          # -- SPIRE agent nodeSelector configuration
          # ref: ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
          nodeSelector: {}
          # -- Security context to be added to spire agent pods.
          # SecurityContext holds pod-level security attributes and common container settings.
          # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
          podSecurityContext: {}
          # -- Security context to be added to spire agent containers.
          # SecurityContext holds pod-level security attributes and common container settings.
          # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
          securityContext: {}
        server:
          # -- SPIRE server image
          image:
            override: ~
            repository: "ghcr.io/spiffe/spire-server"
            tag: "1.8.5"
            digest: "sha256:28269265882048dcf0fed32fe47663cd98613727210b8d1a55618826f9bf5428"
            useDigest: true
            pullPolicy: "IfNotPresent"
          # -- SPIRE server service account
          serviceAccount:
            create: true
            name: spire-server
          # -- SPIRE server init containers
          initContainers: []
          # -- SPIRE server annotations
          annotations: {}
          # -- SPIRE server labels
          labels: {}
          # SPIRE server service configuration
          service:
            # -- Service type for the SPIRE server service
            type: ClusterIP
            # -- Annotations to be added to the SPIRE server service
            annotations: {}
            # -- Labels to be added to the SPIRE server service
            labels: {}
          # -- SPIRE server affinity configuration
          affinity: {}
          # -- SPIRE server nodeSelector configuration
          # ref: ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
          nodeSelector: {}
          # -- SPIRE server tolerations configuration
          # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
          tolerations: []
          # SPIRE server datastorage configuration
          dataStorage:
            # -- Enable SPIRE server data storage
            enabled: true
            # -- Size of the SPIRE server data storage
            size: 1Gi
            # -- Access mode of the SPIRE server data storage
            accessMode: ReadWriteOnce
            # -- StorageClass of the SPIRE server data storage
            storageClass: null
          # -- Security context to be added to spire server pods.
          # SecurityContext holds pod-level security attributes and common container settings.
          # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
          podSecurityContext: {}
          # -- Security context to be added to spire server containers.
          # SecurityContext holds pod-level security attributes and common container settings.
          # ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
          securityContext: {}
          # SPIRE CA configuration
          ca:
            # -- SPIRE CA key type
            # AWS requires the use of RSA. EC cryptography is not supported
            keyType: "rsa-4096"
            # -- SPIRE CA Subject
            subject:
              country: "US"
              organization: "SPIRE"
              commonName: "Cilium SPIRE CA"
      # -- SPIRE server address used by Cilium Operator
      #
      # If k8s Service DNS along with port number is used (e.g. <service-name>.<namespace>.svc(.*):<port-number> format),
      # Cilium Operator will resolve its address by looking up the clusterIP from Service resource.
      #
      # Example values: 10.0.0.1:8081, spire-server.cilium-spire.svc:8081
      serverAddress: ~
      # -- SPIFFE trust domain to use for fetching certificates
      trustDomain: spiffe.cilium
      # -- SPIRE socket path where the SPIRE delegated api agent is listening
      adminSocketPath: /run/spire/sockets/admin.sock
      # -- SPIRE socket path where the SPIRE workload agent is listening.
      # Applies to both the Cilium Agent and Operator
      agentSocketPath: /run/spire/sockets/agent/agent.sock
      # -- SPIRE connection timeout
      connectionTimeout: 30s

